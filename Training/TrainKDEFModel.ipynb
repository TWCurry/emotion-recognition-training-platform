{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TrainKDEFModel.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOo93NWH0co9"
      },
      "source": [
        "# Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JsXS7R-U0h4c"
      },
      "source": [
        "# !pip uninstall terraform -y\r\n",
        "# !pip uninstall terraform-gpu -y\r\n",
        "!pip install tensorflow==2.3.0\r\n",
        "!pip install tensorflow-gpu==2.3.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asYJ-3HPmin2"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oX3tvoONkdhy"
      },
      "source": [
        "import os, gc\r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow import keras\r\n",
        "from tensorflow.keras import layers\r\n",
        "from shutil import copyfile"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3GI6vxdmsP5"
      },
      "source": [
        "# Set up constants\r\n",
        "Set a batch size of 32\r\n",
        "\r\n",
        "Set the image height and width (constant for KDEF dataset)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KE35UFWmrZ1"
      },
      "source": [
        "batchSize = 1\r\n",
        "imgHeight = 762\r\n",
        "imgWidth = 562"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iyr-0Z20nC5a"
      },
      "source": [
        "# Fetch Dataset\r\n",
        "* Mount drive\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sMIJ44QinFnq",
        "outputId": "0d11439b-d05b-4a7e-ee38-d46cfe1228f9"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJ7eXN7eUXHD"
      },
      "source": [
        "* Copy data from drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u78d4NT7UU30",
        "outputId": "8495c242-9ba5-4172-ce6b-bad4cd5f3300"
      },
      "source": [
        "# Create dataset directory if it does not exist\r\n",
        "dataDir = \"/content/dataset\"\r\n",
        "if not (os.path.exists(dataDir)):\r\n",
        "  os.makedirs(dataDir)\r\n",
        "\r\n",
        "# Create emotion directories (if they don't exist)\r\n",
        "emotionCodes = [\"AF\", \"AN\", \"DI\", \"HA\", \"NE\", \"SA\", \"SU\"]\r\n",
        "for emotion in emotionCodes:\r\n",
        "  if not (os.path.exists(os.path.join(dataDir, emotion))):\r\n",
        "    os.makedirs(os.path.join(dataDir, emotion))\r\n",
        "\r\n",
        "# Count files (so we can guess how long it will take to copy over)\r\n",
        "fileCount = 0\r\n",
        "for root, subDirs, files in os.walk(\"/content/gdrive/MyDrive/KDEF\"):\r\n",
        "  for file in files:\r\n",
        "    fileCount +=1\r\n",
        "print(f\"Copying {fileCount} files...\")\r\n",
        "fileCount = 0 # Reusing filecount variable\r\n",
        "\r\n",
        "# Copy files across from Google drive\r\n",
        "for root, subDirs, files in os.walk(\"/content/gdrive/MyDrive/KDEF\"):\r\n",
        "  for file in files:\r\n",
        "    fileCount += 1\r\n",
        "    if len(file) == 11: # Straight ahead image\r\n",
        "      imageDir = \"S\"\r\n",
        "      emotionCode = file[4:-5]\r\n",
        "    else:\r\n",
        "      imageDir = file[6:-4]\r\n",
        "      emotionCode = file[4:-6]\r\n",
        "    copyfile(os.path.join(root, file), os.path.join(dataDir, emotionCode, file))\r\n",
        "    print(f\"\\rCopied {fileCount} images so far\", end=\"\")\r\n",
        "print(\"\\nSuccessfully copied across all images\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying 2938 files...\n",
            "Copied 2938 images so far\n",
            "Successfully copied across all images\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NdfTSabxbYyZ"
      },
      "source": [
        "# Create Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8hpb_dcqbgi-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b511f4bd-7e59-40f1-a98e-870addc0e1c0"
      },
      "source": [
        "dataSetDir = \"/content/dataset\"\r\n",
        "print(f\"Creating dataset from {dataSetDir} directory...\")\r\n",
        "trainingDataset = tf.keras.preprocessing.image_dataset_from_directory(\r\n",
        "  dataSetDir,\r\n",
        "  validation_split=0.2, # 20% of images will be used for validation\r\n",
        "  subset=\"training\",\r\n",
        "  seed=123,\r\n",
        "  image_size=(imgHeight, imgWidth),\r\n",
        "  batch_size=batchSize\r\n",
        ")\r\n",
        "validationDataset = tf.keras.preprocessing.image_dataset_from_directory(\r\n",
        "  dataSetDir,\r\n",
        "  validation_split=0.2, # 20% of images will be used for validation\r\n",
        "  subset=\"validation\",\r\n",
        "  seed=123,\r\n",
        "  image_size=(imgHeight, imgWidth),\r\n",
        "  batch_size=batchSize\r\n",
        ")\r\n",
        "print(f\"Found class names {trainingDataset.class_names}\")\r\n",
        "\r\n",
        "# Configure dataset for performance\r\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\r\n",
        "trainingDataset = trainingDataset.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\r\n",
        "validationDataset = validationDataset.cache().prefetch(buffer_size=AUTOTUNE)\r\n",
        "\r\n",
        "# Standardize data (rescale rgb values from 0-255 to 0-1)\r\n",
        "normalizationLayer = layers.experimental.preprocessing.Rescaling(1./255)\r\n",
        "\r\n",
        "# Create model (model shape and size to be investigated, maybe improved)\r\n",
        "\r\n",
        "classNo = 7\r\n",
        "model = keras.Sequential([\r\n",
        "  layers.experimental.preprocessing.Rescaling(1./255, input_shape=(imgHeight, imgWidth, 3)),\r\n",
        "  layers.Conv2D(16, 3, padding='same', activation='relu'),\r\n",
        "  layers.MaxPooling2D(),\r\n",
        "  layers.Conv2D(32, 3, padding='same', activation='relu'),\r\n",
        "  layers.MaxPooling2D(),\r\n",
        "  layers.Conv2D(64, 3, padding='same', activation='relu'),\r\n",
        "  layers.MaxPooling2D(),\r\n",
        "  layers.Flatten(),\r\n",
        "  layers.Dense(128, activation='relu'),\r\n",
        "  layers.Dense(classNo)    \r\n",
        "])\r\n",
        "\r\n",
        "# Compile model\r\n",
        "model.compile(\r\n",
        "  optimizer='adam',\r\n",
        "  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\r\n",
        "  metrics=['accuracy']\r\n",
        ")\r\n",
        "print(\"Model summary:\")\r\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Creating dataset from /content/dataset directory...\n",
            "Found 2938 files belonging to 7 classes.\n",
            "Using 2351 files for training.\n",
            "Found 2938 files belonging to 7 classes.\n",
            "Using 587 files for validation.\n",
            "Found class names ['AF', 'AN', 'DI', 'HA', 'NE', 'SA', 'SU']\n",
            "Model summary:\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "rescaling_1 (Rescaling)      (None, 762, 562, 3)       0         \n",
            "_________________________________________________________________\n",
            "conv2d (Conv2D)              (None, 762, 562, 16)      448       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 381, 281, 16)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 381, 281, 32)      4640      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 190, 140, 32)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 190, 140, 64)      18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 95, 70, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 425600)            0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               54476928  \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 7)                 903       \n",
            "=================================================================\n",
            "Total params: 54,501,415\n",
            "Trainable params: 54,501,415\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnIPOnbicwQe"
      },
      "source": [
        "# Train model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9uJFrEbcyt5",
        "outputId": "dafc9cf4-7250-4314-d4e3-a2b72ee0fd1a"
      },
      "source": [
        "# Manually run garbage collection to clear RAM\r\n",
        "gc.collect()\r\n",
        "\r\n",
        "# Train model\r\n",
        "epochs=5\r\n",
        "history = model.fit(\r\n",
        "  trainingDataset,\r\n",
        "  validation_data=validationDataset,\r\n",
        "  epochs=epochs\r\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "1155/2351 [=============>................] - ETA: 13:32 - loss: 2.2389 - accuracy: 0.1229"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}